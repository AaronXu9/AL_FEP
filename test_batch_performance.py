#!/usr/bin/env python3
"""
Test script to demonstrate batch processing performance improvement
in a realistic active learning scenario.
"""

import os
import sys
import time
import numpy as np
import logging
from typing import List, Dict, Any

# Add the src directory to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from al_fep.oracles.docking_oracle import DockingOracle

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def generate_test_molecules(n: int) -> List[str]:
    """Generate test molecules for evaluation."""
    # Common drug-like fragments and scaffolds
    base_smiles = [
        "CCO",  # Ethanol
        "CC(C)CC1=CC=C(C=C1)C(C)C(=O)O",  # Ibuprofen
        "CN1C=NC2=C1C(=O)N(C(=O)N2C)C",  # Caffeine  
        "CC(=O)OC1=CC=CC=C1C(=O)O",  # Aspirin
        "CC1=CC=C(C=C1)C2=CC(=NN2C3=CC=C(C=C3)S(=O)(=O)N)C(F)(F)F",  # Celecoxib
        "CC1=CC=C(C=C1)C(=O)C2=CC=C(C=C2)OC",  # Flavone
        "C1=CC=C(C=C1)C2=CC=C(C=C2)O",  # Biphenyl
        "CN1CCN(CC1)C2=C(C=C(C=C2)Cl)C(=O)O",  # Aniline derivative
        "CC1=CC(=CC=C1)C(=O)NC2=CC=CC=C2",  # Benzamide
        "C1=CC=C(C=C1)C(=O)N2CCCC2",  # Pyrrolidine
    ]
    
    # Generate variations by randomly selecting from base molecules
    np.random.seed(42)
    molecules = []
    for i in range(n):
        base = base_smiles[i % len(base_smiles)]
        molecules.append(base)
    
    return molecules

def benchmark_evaluation_methods():
    """Benchmark individual vs batch evaluation."""
    print("=== Batch Processing Performance Benchmark ===")
    
    # Test different batch sizes
    batch_sizes = [1, 5, 10, 20, 50]
    num_molecules = 50
    
    # Generate test molecules
    test_molecules = generate_test_molecules(num_molecules)
    
    # Configuration for mock testing
    config = {
        "docking": {
            "engine": "gnina",
            "mock_mode": True,
            "center_x": 0.0,
            "center_y": 0.0, 
            "center_z": 0.0,
            "size_x": 20.0,
            "size_y": 20.0,
            "size_z": 20.0,
            "exhaustiveness": 8,
            "num_poses": 3
        }
    }
    
    print(f"Testing with {num_molecules} molecules")
    print("=" * 60)
    
    results_summary = []
    
    for batch_size in batch_sizes:
        print(f"\nBatch size: {batch_size}")
        print("-" * 30)
        
        # Initialize oracle for each test
        oracle = DockingOracle(target="benchmark", config=config)
        
        # Time the evaluation
        start_time = time.time()
        
        if batch_size == 1:
            # Individual evaluation (one by one)
            evaluation_results = []
            for smiles in test_molecules:
                result = oracle.evaluate(smiles)
                evaluation_results.append(result)
            method = "Individual"
        else:
            # Batch evaluation
            if batch_size >= len(test_molecules):
                # Single large batch
                evaluation_results = oracle.evaluate(test_molecules)
                method = "Single Batch"
            else:
                # Multiple smaller batches
                evaluation_results = []
                for i in range(0, len(test_molecules), batch_size):
                    batch = test_molecules[i:i + batch_size]
                    batch_results = oracle.evaluate(batch)
                    evaluation_results.extend(batch_results)
                method = "Multi Batch"
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # Calculate statistics
        # Handle both single dict and list of dicts
        if isinstance(evaluation_results, list):
            successful_evaluations = sum(1 for result in evaluation_results if result.get('score') is not None)
        else:
            # Single result
            successful_evaluations = 1 if evaluation_results.get('score') is not None else 0
            evaluation_results = [evaluation_results]  # Convert to list for consistency
        avg_time_per_molecule = total_time / len(test_molecules)
        
        # Get oracle statistics
        stats = oracle.get_statistics()
        
        # Store results
        result_data = {
            'batch_size': batch_size,
            'method': method,
            'total_time': total_time,
            'avg_time_per_mol': avg_time_per_molecule,
            'successful': successful_evaluations,
            'total_molecules': len(test_molecules),
            'oracle_calls': stats['call_count'],
            'supports_batch': oracle.supports_batch_processing()
        }
        results_summary.append(result_data)
        
        print(f"Method: {method}")
        print(f"Total time: {total_time:.3f}s")
        print(f"Avg time per molecule: {avg_time_per_molecule:.4f}s")
        print(f"Successful evaluations: {successful_evaluations}/{len(test_molecules)}")
        print(f"Oracle calls: {stats['call_count']}")
        print(f"Supports batch processing: {oracle.supports_batch_processing()}")
    
    # Summary comparison
    print("\n" + "=" * 60)
    print("PERFORMANCE SUMMARY")
    print("=" * 60)
    
    individual_time = results_summary[0]['total_time']
    
    print(f"{'Batch Size':<12} {'Method':<15} {'Time (s)':<10} {'Speedup':<10} {'Success Rate':<12}")
    print("-" * 65)
    
    for result in results_summary:
        speedup = individual_time / result['total_time'] if result['total_time'] > 0 else float('inf')
        success_rate = result['successful'] / result['total_molecules'] * 100
        
        print(f"{result['batch_size']:<12} {result['method']:<15} {result['total_time']:<10.3f} "
              f"{speedup:<10.2f} {success_rate:<12.1f}%")
    
    return results_summary

def test_active_learning_scenario():
    """Test batch processing in an active learning-like scenario."""
    print("\n\n=== Active Learning Scenario Test ===")
    
    # Simulate an active learning workflow
    total_molecules = 100
    initial_batch_size = 5
    al_batch_size = 10
    num_rounds = 5
    
    molecules = generate_test_molecules(total_molecules)
    
    config = {
        "docking": {
            "engine": "gnina", 
            "mock_mode": True,
            "center_x": 0.0,
            "center_y": 0.0,
            "center_z": 0.0,
            "size_x": 20.0,
            "size_y": 20.0,
            "size_z": 20.0
        }
    }
    
    oracle = DockingOracle(target="al_test", config=config)
    
    print(f"Dataset: {total_molecules} molecules")
    print(f"Initial batch: {initial_batch_size}, AL batch: {al_batch_size}, Rounds: {num_rounds}")
    print(f"Oracle supports batch processing: {oracle.supports_batch_processing()}")
    
    total_evaluated = 0
    total_time = 0
    
    # Initial batch
    print(f"\nRound 0: Initial batch ({initial_batch_size} molecules)")
    start_time = time.time()
    initial_batch = molecules[:initial_batch_size]
    initial_results = oracle.evaluate(initial_batch)  # This returns a list
    batch_time = time.time() - start_time
    
    successful = sum(1 for r in initial_results if r.get('score') is not None)
    total_evaluated += len(initial_batch)
    total_time += batch_time
    
    print(f"  Time: {batch_time:.3f}s, Success: {successful}/{len(initial_batch)}")
    
    # Active learning rounds
    current_idx = initial_batch_size
    
    for round_num in range(1, num_rounds + 1):
        if current_idx >= total_molecules:
            print(f"Round {round_num}: No more molecules available")
            break
            
        # Select next batch
        end_idx = min(current_idx + al_batch_size, total_molecules)
        batch = molecules[current_idx:end_idx]
        
        print(f"Round {round_num}: Evaluating {len(batch)} molecules")
        
        start_time = time.time()
        batch_results = oracle.evaluate(batch)  # This returns a list
        batch_time = time.time() - start_time
        
        successful = sum(1 for r in batch_results if r.get('score') is not None)
        total_evaluated += len(batch)
        total_time += batch_time
        
        print(f"  Time: {batch_time:.3f}s, Success: {successful}/{len(batch)}")
        
        current_idx = end_idx
    
    # Final statistics
    stats = oracle.get_statistics()
    avg_time_per_mol = total_time / total_evaluated if total_evaluated > 0 else 0
    
    print(f"\nFinal Results:")
    print(f"Total molecules evaluated: {total_evaluated}")
    print(f"Total time: {total_time:.3f}s")
    print(f"Average time per molecule: {avg_time_per_mol:.4f}s")
    print(f"Oracle calls: {stats['call_count']}")
    print(f"Oracle avg time: {stats['avg_time']:.4f}s")

if __name__ == "__main__":
    try:
        benchmark_results = benchmark_evaluation_methods()
        test_active_learning_scenario()
        
        print("\nðŸŽ‰ Batch processing benchmark completed successfully!")
        print("\nKey Benefits:")
        print("âœ… Batch processing reduces total evaluation time")
        print("âœ… Resource utilization is more efficient")  
        print("âœ… Perfect for active learning workflows")
        print("âœ… Maintains result accuracy and error handling")
        
    except Exception as e:
        logger.error(f"Benchmark failed with error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
